{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[1869]: Class CaptureDelegate is implemented in both /Users/nuyhc/miniforge3/envs/Deep/lib/python3.8/site-packages/mediapipe/.dylibs/libopencv_videoio.3.4.16.dylib (0x107d88860) and /Users/nuyhc/miniforge3/envs/Deep/lib/python3.8/site-packages/cv2/cv2.abi3.so (0x15b84a538). One of the two will be used. Which one is undefined.\n",
      "objc[1869]: Class CVWindow is implemented in both /Users/nuyhc/miniforge3/envs/Deep/lib/python3.8/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x1077b4a68) and /Users/nuyhc/miniforge3/envs/Deep/lib/python3.8/site-packages/cv2/cv2.abi3.so (0x15b84a588). One of the two will be used. Which one is undefined.\n",
      "objc[1869]: Class CVView is implemented in both /Users/nuyhc/miniforge3/envs/Deep/lib/python3.8/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x1077b4a90) and /Users/nuyhc/miniforge3/envs/Deep/lib/python3.8/site-packages/cv2/cv2.abi3.so (0x15b84a5b0). One of the two will be used. Which one is undefined.\n",
      "objc[1869]: Class CVSlider is implemented in both /Users/nuyhc/miniforge3/envs/Deep/lib/python3.8/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x1077b4ab8) and /Users/nuyhc/miniforge3/envs/Deep/lib/python3.8/site-packages/cv2/cv2.abi3.so (0x15b84a5d8). One of the two will be used. Which one is undefined.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import pytube\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import imageio.v2 as imageio\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawling Data\n",
    "틱톡 영상으로 테스트  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_video(url, res=\"720p\", path=\"./\"):\n",
    "    yt = pytube.YouTube(url)\n",
    "    stream = yt.streams.filter(res=res).first()\n",
    "    stream.download(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [블랙핑크] 요즘 유행인 지글지글 춤 in 영국 대사관 24s\n",
    "download_video(\"https://www.youtube.com/watch?v=JfGFx9tDVpc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KeyPoint 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_to_gif(frames, output_name):\n",
    "    images = []\n",
    "    for frame in frames:\n",
    "        images.append(imageio.imread(frame))\n",
    "    imageio.mimsave(output_path+output_name+\".gif\", images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spec3\\AppData\\Local\\Temp\\ipykernel_13772\\2401414973.py:4: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning dissapear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  images.append(imageio.imread(frame))\n"
     ]
    }
   ],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_style = mp.solutions.drawing_styles\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# TODO: OS 모듈로 영상 별로 이미지 디렉토리 만들고 저장 할 수 있게 변경\n",
    "video_name = \"220421 아이브 안유진 직캠 LOVE DIVE (IVE YUJIN FanCam)  @MCOUNTDOWN_2022421.mp4\"\n",
    "video_path = \"./\" + video_name\n",
    "\n",
    "# 비디오 로드\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "# 프레임 별로 잘린 이미지\n",
    "frames_path = \"./keypoint_extraction/frames/\"\n",
    "# gif\n",
    "output_path = \"./keypoint_extraction/output/\"\n",
    "\n",
    "annotate_frames = []\n",
    "keypoint_dict = []\n",
    "\n",
    "# 바로 윈도우 열리고 출력 됨\n",
    "i = 0\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Empty Frame\")\n",
    "            break\n",
    "        # 이미지 반전 및 BGR -> RGB\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        results = pose.process(image)\n",
    "        \n",
    "        if results.pose_landmarks is not None:\n",
    "            annotated_pose_landmarks = {str(j): [lmk.x, lmk.y, lmk.z] for j, lmk in enumerate(results.pose_landmarks.landmark)}\n",
    "            keypoint_dict.append(annotated_pose_landmarks)\n",
    "        \n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS, landmark_drawing_spec=mp_drawing_style.get_default_pose_landmarks_style())\n",
    "        cv2.imwrite(frames_path+str(i)+\".png\", image)\n",
    "        annotate_frames.append(frames_path+str(i)+\".png\")\n",
    "        i += 1\n",
    "        cv2.imshow(\"Pose KeyPoint Extract: \"+video_name, image)\n",
    "        if cv2.waitKey(5) & 0xFF == 27: break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# gif 형식으로 저장\n",
    "frames_to_gif(annotate_frames, video_name)\n",
    "\n",
    "# 키포인트 json 형식으로 저장\n",
    "with open(output_path+video_name+\"_keypoints.json\", \"w\") as fp:\n",
    "    json.dump(keypoint_dict, fp)\n",
    "    \n",
    "# 대략 2분 정도 걸리는 듯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pose Estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# 동작의 좌표를 변환하는 메커니즘\n",
    "# 두 포즈를 비교하는 L2 규제가 효과적이라고 함\n",
    "def l2_norm(ground_relative_coords, webcam_relative_coords):\n",
    "\treturn np.linalg.norm(ground_relative_coords - webcam_relative_coords)\n",
    "\n",
    "def print_data(ground_points, webcam_points, translation_factors, w, h):\n",
    "    print(ground_points[str(11)][0:2] * np.array([w, h]) - np.array(list(translation_factors)))\n",
    "    print(webcam_points[11][0:2]* np.array([w, h]))\n",
    "\n",
    "def compare_keypoints(ground_points, webcam_points, w, h, translation_factors):\n",
    "\tground_points_array = []\n",
    "\twebcam_points_array = []\n",
    "    \n",
    "\tfor i in range(len(ground_points)):\n",
    "        # 일단 공간은 생각 안하고 x, y만 고려\n",
    "\t\tground_points_array.append(np.array(ground_points[str(i)])[0:2]* np.array([w, h]) - np.array(list(translation_factors)))\n",
    "\t\twebcam_points_array.append(np.array(webcam_points[i])[0:2]* np.array([w, h]))\n",
    "\n",
    "\tground_points_array = np.vstack(ground_points_array)\n",
    "\twebcam_points_array = np.vstack(webcam_points_array)\n",
    "\n",
    "\treturn l2_norm(ground_points_array, webcam_points_array)\n",
    "\n",
    "def connect_points(points, translation_factors, image, image_shape, scale):\n",
    "    h, w = image_shape\n",
    "    points_connect_dict = {\n",
    "        1: [2, 0],\n",
    "        2: [3],\n",
    "        3: [7],\n",
    "        4: [0, 5],\n",
    "        5: [6],\n",
    "        6: [8],\n",
    "        9: [10],\n",
    "        11: [13],\n",
    "        12: [11, 14],\n",
    "        13: [15],\n",
    "        14: [16],\n",
    "        15: [21],\n",
    "        16: [20, 14],\n",
    "        17: [15],\n",
    "        18: [20, 16],\n",
    "        19: [17],\n",
    "        20: [16],\n",
    "        22: [16],\n",
    "        23: [11, 25],\n",
    "        24: [23, 12],\n",
    "        25: [27],\n",
    "        26: [24, 28],\n",
    "        27: [31, 29],\n",
    "        28: [30, 32],\n",
    "        29: [31],\n",
    "        30: [32],\n",
    "        32: [28],\n",
    "    }\n",
    "    for p in points_connect_dict:\n",
    "        curr_point = points[str(p)][0:2]*np.array([w, h]) - np.array(list(translation_factors))\n",
    "\n",
    "        for endpoint in points_connect_dict[p]:\n",
    "            endpoint = points[str(endpoint)][0:2]*np.array([w, h]) - np.array(list(translation_factors))\n",
    "\n",
    "            cv2.line(image, (round(curr_point[0]*scale), round(curr_point[1]*scale)), (round(endpoint[0] * scale), round(endpoint[1] * scale)), (0, 0, 255), thickness=10)\n",
    "\n",
    "    return image\n",
    "\n",
    "def get_translation_factor(gt, person, h, w):\n",
    "    x_gt, y_gt = gt['11'][0]*w, gt['11'][1]*h\n",
    "    x_person, y_person = person[11][0]*w, person[11][1]*h\n",
    "\n",
    "    if x_person >= x_gt:\n",
    "        return x_person - x_gt, y_person - y_gt\n",
    "    elif x_person <= x_gt:\n",
    "        return x_gt - x_person, y_gt - y_person\n",
    "\n",
    "\n",
    "def put_text(image, text, h, w):\n",
    "    image = cv2.putText(img=image, org=(w - 700, 50), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=2, color=(0, 0, 0), text=text, thickness= 3)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Input Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_style = mp.solutions.drawing_styles\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "cv2.startWindowThread()\n",
    "# 비디오 로드\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "video_name = \"user_input_test\"\n",
    "# 프레임 별로 잘린 이미지\n",
    "frames_path = \"./keypoint_extraction/frames/\"\n",
    "# gif\n",
    "output_path = \"./keypoint_extraction/output/\"\n",
    "\n",
    "annotate_frames = []\n",
    "keypoint_dict = []\n",
    "\n",
    "# 바로 윈도우 열리고 출력 됨\n",
    "i = 0\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Empty Frame\")\n",
    "            break\n",
    "        # 이미지 반전 및 BGR -> RGB\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        results = pose.process(image)\n",
    "        \n",
    "        if results.pose_landmarks is not None:\n",
    "            annotated_pose_landmarks = {str(j): [lmk.x, lmk.y, lmk.z] for j, lmk in enumerate(results.pose_landmarks.landmark)}\n",
    "            keypoint_dict.append(annotated_pose_landmarks)\n",
    "        \n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS, landmark_drawing_spec=mp_drawing_style.get_default_pose_landmarks_style())\n",
    "        cv2.imwrite(frames_path+str(i)+\".png\", image)\n",
    "        annotate_frames.append(frames_path+str(i)+\".png\")\n",
    "        i += 1\n",
    "        cv2.imshow(\"Pose KeyPoint Extract: \"+video_name, image)\n",
    "        if cv2.waitKey(5) & 0xFF == 27: break\n",
    "        \n",
    "cap.release()\n",
    "cv2.waitKey(1)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n",
    "\n",
    "# gif 형식으로 저장\n",
    "frames_to_gif(annotate_frames, video_name)\n",
    "\n",
    "# 키포인트 json 형식으로 저장\n",
    "with open(output_path+video_name+\"_keypoints.json\", \"w\") as fp:\n",
    "    json.dump(keypoint_dict, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 원본 영상에서 추출한 키포인트와, 입력으로 입력한 키포인트를 비교하는 방법\n",
    "2. 추출한 키포인트를 사용자 웹캠 이미지에 덮어 씌우는 방법\n",
    "3. 1, 2를 합쳐서 연습과 스코어링\n",
    "\n",
    "사용자 입력을 받는 경우와 영상에서 추출하는 경우 불필요한 부분을 제외할 방법을 고민해봐야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[1841]: Class CaptureDelegate is implemented in both /Users/nuyhc/miniforge3/envs/Deep/lib/python3.8/site-packages/mediapipe/.dylibs/libopencv_videoio.3.4.16.dylib (0x1103c8860) and /Users/nuyhc/miniforge3/envs/Deep/lib/python3.8/site-packages/cv2/cv2.abi3.so (0x16650e538). One of the two will be used. Which one is undefined.\n",
      "objc[1841]: Class CVWindow is implemented in both /Users/nuyhc/miniforge3/envs/Deep/lib/python3.8/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x106fdca68) and /Users/nuyhc/miniforge3/envs/Deep/lib/python3.8/site-packages/cv2/cv2.abi3.so (0x16650e588). One of the two will be used. Which one is undefined.\n",
      "objc[1841]: Class CVView is implemented in both /Users/nuyhc/miniforge3/envs/Deep/lib/python3.8/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x106fdca90) and /Users/nuyhc/miniforge3/envs/Deep/lib/python3.8/site-packages/cv2/cv2.abi3.so (0x16650e5b0). One of the two will be used. Which one is undefined.\n",
      "objc[1841]: Class CVSlider is implemented in both /Users/nuyhc/miniforge3/envs/Deep/lib/python3.8/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x106fdcab8) and /Users/nuyhc/miniforge3/envs/Deep/lib/python3.8/site-packages/cv2/cv2.abi3.so (0x16650e5d8). One of the two will be used. Which one is undefined.\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "output_path = \"./\"\n",
    "video_name = \"user_input\"\n",
    "video_path = \"./\"+video_name\n",
    "\n",
    "# Open WebCam (Suppose Only 2 Maximum WebCam Install)\n",
    "cv2.startWindowThread()\n",
    "try:\n",
    "    cap = cv2.VideoCapture(0)\n",
    "except:\n",
    "    cap = cv2.VideoCapture(1)\n",
    "# 720p\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "keypoint_face = []\n",
    "# keypoint_left_hand = []\n",
    "# keypoint_right_hand = []\n",
    "keypoint_pose = []\n",
    "\n",
    "# Init Holistic Model    \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Video Error\")\n",
    "            break\n",
    "        \n",
    "        # BGR -> RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        # Make Detections\n",
    "        results = holistic.process(image)\n",
    "\n",
    "        # Collect Keypoint coord.\n",
    "        if results.face_landmarks is not None:\n",
    "            annotated_face_landmarks = {str(idx): [lmk.x, lmk.y, lmk.z] for idx, lmk in enumerate(results.face_landmarks.landmark)}\n",
    "            keypoint_face.append(annotated_face_landmarks)\n",
    "        # if results.left_hand_landmarks is not None:\n",
    "        #     annotated_left_hand_landmarks = {str(idx): [lmk.x, lmk.y, lmk.z] for idx, lmk in enumerate(results.left_hand_landmarks.landmark)}\n",
    "        # if results.right_hand_landmarks is not None:\n",
    "        #     annotated_right_hand_landmarks = {str(idx): [lmk.x, lmk.y, lmk.z] for idx, lmk in enumerate(results.right_hand_landmarks.landmark)}\n",
    "        if results.pose_landmarks is not None:\n",
    "            annotated_pose_landmarks = {str(idx): [lmk.x, lmk.y, lmk.z] for idx, lmk in enumerate(results.pose_landmarks.landmark)}\n",
    "            keypoint_pose.append(annotated_pose_landmarks)\n",
    "        \n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        # Drawing Landmarks on Realtime\n",
    "        mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS,\n",
    "                                  landmark_drawing_spec=mp_drawing.DrawingSpec(color=(255, 255, 0), thickness=1, circle_radius=1),\n",
    "                                  connection_drawing_spec=mp_drawing.DrawingSpec(color=(255, 255, 204), thickness=1, circle_radius=1))\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                                  landmark_drawing_spec=mp_drawing.DrawingSpec(color=(255, 51, 51), thickness=2, circle_radius=2),\n",
    "                                  connection_drawing_spec=mp_drawing.DrawingSpec(color=(255, 204, 204), thickness=1, circle_radius=1))\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                                  landmark_drawing_spec=mp_drawing.DrawingSpec(color=(255, 51, 51), thickness=2, circle_radius=2),\n",
    "                                  connection_drawing_spec=mp_drawing.DrawingSpec(color=(255, 204, 204), thickness=1, circle_radius=1))\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                                  landmark_drawing_spec=mp_drawing.DrawingSpec(color=(102, 102, 255), thickness=2, circle_radius=2),\n",
    "                                  connection_drawing_spec=mp_drawing.DrawingSpec(color=(153, 153, 255), thickness=1, circle_radius=1))\n",
    "        \n",
    "        cv2.imshow(\"KeyPoint Extraction\", cv2.flip(image, 1))\n",
    "        \n",
    "        if cv2.waitKey(10)&0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.waitKey(1)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_norm(original_coor, compare_coor):\n",
    "    return np.linalg.norm(original_coor - compare_coor)\n",
    "\n",
    "def compare_origin_input(original_coor, compare_coor, w, h):\n",
    "    ori_arr = []\n",
    "    com_arr = []\n",
    "\n",
    "    for i in range(len(original_coor)):\n",
    "        ori_arr.append(np.array(original_coor[str(i)])[0:2] * np.array([w, h]))\n",
    "        com_arr.append(np.array(compare_coor[str(i)])[0:2] * np.array([w, h]))\n",
    "    ori_arr = np.vstack(ori_arr)\n",
    "    com_arr = np.vstack(com_arr)\n",
    "    return l2_norm(ori_arr, com_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1_path = \"../template/[주간아 직캠] IVE YUJIN - LOVE DIVE (아이브 유진 - 러브 다이브) l EP556.mp4_keypoints.json\"\n",
    "input_2_path = \"../template/220421 아이브 안유진 직캠 LOVE DIVE (IVE YUJIN FanCam)  @MCOUNTDOWN_2022421.mp4_keypoints.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_json(input_1_path)\n",
    "data2 = pd.read_json(input_2_path)\n",
    "\n",
    "pose_landmark_dict = {\n",
    "    # Face ###################################\n",
    "    0: \"nose\",\n",
    "    # eyes\n",
    "    1: \"left_eye_inner\",\n",
    "    2: \"left_eye\",\n",
    "    3: \"left_eye_outer\",\n",
    "    4: \"right_eye_inner\",\n",
    "    5: \"right_eye\",\n",
    "    6: \"right_eye_outer\",\n",
    "    # ears\n",
    "    7: \"left_ear\",\n",
    "    8: \"right_ear\",\n",
    "    # mouth\n",
    "    9: \"left_mouth\",\n",
    "    10: \"right_mouth\",\n",
    "    # Body ###################################\n",
    "    # shoulder\n",
    "    11: \"left_shoulder\",\n",
    "    12: \"right_shoulder\",\n",
    "    # elbow\n",
    "    13: \"left_elbow\",\n",
    "    14: \"right_elbow\",\n",
    "    # wrist\n",
    "    15: \"left_wrist\",\n",
    "    16: \"right_wrist\",\n",
    "    # hand\n",
    "    17: \"left_pinky\",\n",
    "    18: \"right_pinky\",\n",
    "    19: \"left_index\",\n",
    "    20: \"right_index\",\n",
    "    21: \"left_thumb\",\n",
    "    22: \"right_thumb\",\n",
    "    # hip\n",
    "    23: \"left_hip\",\n",
    "    24: \"right_hip\",\n",
    "    # knee\n",
    "    25: \"left_knee\",\n",
    "    26: \"right_knee\",\n",
    "    # ankle\n",
    "    27: \"left_ankle\",\n",
    "    28: \"right_ankle\",\n",
    "    # heel\n",
    "    29: \"left_heel\",\n",
    "    30: \"right_heel\",\n",
    "    # foot\n",
    "    31: \"left_foot_index\",\n",
    "    32: \"right_foot_index\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.columns = pose_landmark_dict.values()\n",
    "data2.columns = pose_landmark_dict.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.470056474208831, 0.26909977197647, -0.031963795423507003]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.loc[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.28000330924987704, 0.13502180576324402, -0.007499510888010001]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.loc[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'list'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/nuyhc/Desktop/Dev/Just-DDance/Detection/dance_pose.ipynb 셀 21\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nuyhc/Desktop/Dev/Just-DDance/Detection/dance_pose.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpairwise\u001b[39;00m \u001b[39mimport\u001b[39;00m cosine_similarity\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nuyhc/Desktop/Dev/Just-DDance/Detection/dance_pose.ipynb#X36sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m cosine_similarity(data1\u001b[39m.\u001b[39;49mto_numpy(), data2\u001b[39m.\u001b[39;49mto_numpy())\n",
      "File \u001b[0;32m~/miniforge3/envs/Deep/lib/python3.8/site-packages/sklearn/metrics/pairwise.py:1351\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1317\u001b[0m \u001b[39m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[1;32m   1318\u001b[0m \n\u001b[1;32m   1319\u001b[0m \u001b[39mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[39mkernel matrix : ndarray of shape (n_samples_X, n_samples_Y)\u001b[39;00m\n\u001b[1;32m   1348\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1349\u001b[0m \u001b[39m# to avoid recursive import\u001b[39;00m\n\u001b[0;32m-> 1351\u001b[0m X, Y \u001b[39m=\u001b[39m check_pairwise_arrays(X, Y)\n\u001b[1;32m   1353\u001b[0m X_normalized \u001b[39m=\u001b[39m normalize(X, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1354\u001b[0m \u001b[39mif\u001b[39;00m X \u001b[39mis\u001b[39;00m Y:\n",
      "File \u001b[0;32m~/miniforge3/envs/Deep/lib/python3.8/site-packages/sklearn/metrics/pairwise.py:156\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[1;32m    147\u001b[0m     X \u001b[39m=\u001b[39m Y \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m    148\u001b[0m         X,\n\u001b[1;32m    149\u001b[0m         accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m         estimator\u001b[39m=\u001b[39mestimator,\n\u001b[1;32m    154\u001b[0m     )\n\u001b[1;32m    155\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m    157\u001b[0m         X,\n\u001b[1;32m    158\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[1;32m    159\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    160\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    161\u001b[0m         force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[1;32m    162\u001b[0m         estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    164\u001b[0m     Y \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m    165\u001b[0m         Y,\n\u001b[1;32m    166\u001b[0m         accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    170\u001b[0m         estimator\u001b[39m=\u001b[39mestimator,\n\u001b[1;32m    171\u001b[0m     )\n\u001b[1;32m    173\u001b[0m \u001b[39mif\u001b[39;00m precomputed:\n",
      "File \u001b[0;32m~/miniforge3/envs/Deep/lib/python3.8/site-packages/sklearn/utils/validation.py:856\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    854\u001b[0m         array \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mastype(dtype, casting\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39munsafe\u001b[39m\u001b[39m\"\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    855\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 856\u001b[0m         array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m    857\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[1;32m    858\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    859\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[1;32m    860\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarity(data1.to_numpy(), data2.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = min(data1.shape, data2.shape)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.28000330924987704, 0.13502180576324402, -0.007499510888010001]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.to_numpy()[0][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('Deep')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2687cd7e348f8b276a3d3877a91949627292e389c8a2a645f881205e3ac80541"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
